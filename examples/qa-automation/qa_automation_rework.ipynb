{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LaVague for QA Automation\n",
    "\n",
    "In this notebook, we'll look at how LaVague can be used to generate automated tests from simple test scenario.\n",
    "\n",
    "We will use LaVague to autonomously run the test and record xpath and actions. We'll then use an LLM to generate assert statements and the final reusable test file. \n",
    "\n",
    "We first define our test with the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.amazon.com/\" # the URL you want to run your test on\n",
    "\n",
    "FEATURE_FILE_NAME = \"amazon_cart.feature\" # name of the feature file that will be generated\n",
    "PYTEST_FILE_NAME = \"amazon_cart_test.py\" # name of the pytest file that will be generated\n",
    "\n",
    "# the high level test case that will be used to generate the feature file and the test file\n",
    "HIGH_LEVEL_TEST_CASE = \"\"\"\n",
    "Search for `1984 75th anniversary edition` and press Enter. \n",
    "Add it to cart and verify that the cart feature works\n",
    "\"\"\"\n",
    "\n",
    "OBJECTIVE = f\"Perform the following test:\\n {HIGH_LEVEL_TEST_CASE}\" # the objective we give LaVague\n",
    "\n",
    "# We could make this framework agnostic by rewriting the prompt and examples passed to the multi-modal LLM\n",
    "# TARGET_FRAMEWORK = \"pytest-bdd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LaVague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from lavague.core import  WorldModel, ActionEngine\n",
    "from lavague.core.agents import WebAgent\n",
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.legacy.readers.file.base import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a standard LaVague agent\n",
    "selenium_driver = SeleniumDriver(headless=False)\n",
    "world_model = WorldModel()\n",
    "action_engine = ActionEngine(selenium_driver)\n",
    "agent = WebAgent(world_model, action_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the URL and manually pass CAPTCHA before running the agent (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get(\"https://www.amazon.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the agent\n",
    "Run the high level test case once. Get logs as a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(OBJECTIVE, log_to_db=True)\n",
    "\n",
    "logs = agent.logger.return_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract navigation data from the agent run\n",
    "### Get relevant nodes to generate the assert statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = action_engine.navigation_engine.get_nodes(\n",
    "    f\"We have ran the test case, generate the final assert statement.\\n\\ntest case:\\n{HIGH_LEVEL_TEST_CASE}\"\n",
    ")\n",
    "\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all actions performed and screenshot of last page state\n",
    "\n",
    "From logs, clean the \"code\" column to remove CoT comments then create a new dataframe that contains each instruction with the associated xpath and action taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all actions and cleaup\n",
    "actions = \"\\n\".join(logs[\"code\"].dropna())\n",
    "\n",
    "def remove_comments(code):\n",
    "    return '\\n'.join([line for line in code.split('\\n') if not line.strip().startswith('#')])\n",
    "\n",
    "logs['action'] = logs['code'].dropna().apply(remove_comments)\n",
    "cleaned_logs = logs[['instruction', 'action']].fillna('')\n",
    "actions = '\\n\\n'.join(cleaned_logs['instruction'] + ' ' + cleaned_logs['action'])\n",
    "\n",
    "instructions = \"\\n\".join(cleaned_logs['instruction'])\n",
    "print(actions)\n",
    "\n",
    "# get last page screenshot\n",
    "last_page_screenshot = SimpleDirectoryReader(logs.iloc[-1][\"screenshots_path\"]).load_data() # load last screenshot taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output generation\n",
    "Use all gathered data about the site to generate a Gherkin, then a pytest-bdd file\n",
    "\n",
    "We'll first create two LLMs\n",
    "- multi modal LLM `gpt-4o` is used to generate the final code file (we pass a screenshot of the last page state to help with the assert generation)\n",
    "- text only LLM `gpt-4` to generate the feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o = OpenAIMultiModal(\"gpt-4o\")\n",
    "gpt4o.max_new_tokens = 2000 # 300 by default, we increase it to make sure our pytest file doesn't get trucated\n",
    "\n",
    "gpt4 = OpenAI(\"gpt-4\")\n",
    "\n",
    "# we'll clean the triple quote answer from the LLMs\n",
    "def clean_output(markdown_code_block):\n",
    "    return markdown_code_block.replace(\"```python\", \"\").replace(\"```\", \"\").replace(\"```\\n\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Gherkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_GHERKIN = f\"\"\"\n",
    "Generate a Gherkin-style test case for the following initial objective and actions already ran by the agent. \n",
    "Only output in a markdown triple quoted string and nothing else.\n",
    "Initial objective = {OBJECTIVE}\\n\\nInstructions ran: {instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_gherkin = gpt4.complete(PROMPT_GHERKIN).text\n",
    "generated_gherkin = clean_output(generated_gherkin)\n",
    "print(generated_gherkin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pytest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an expert in software testing frameworks and Python code generation. You answer in python markdown only and nothing else, don't include anything after the last backticks. Your task is to:\n",
    "1. Process a test case, existing actions (with their associated xpath) that was ran during the test by an agent, relevant HTML nodes, and the screenshot of the last state of the page.\n",
    "2. Generate an assert statement for the last condition of the test case. THis is the most important, you need to generate asserts at all costs\n",
    "3. Package everything in a pytest-bdd file following best practices. \n",
    "Requirements:\n",
    "- Use descriptive function names and name the scenario appropriately.\n",
    "- Use execute_script to handle potential ElementClickInterceptedException during clicks.\n",
    "- Include fixtures, scenario, Gherkin-style definitions, etc.\n",
    "- Use try-except blocks to catch exceptions and raise pytest.fail for assert condition steps as needed.\n",
    "- Do not generate asserts for 'Given', 'When', or 'And' steps; only generate asserts for 'Then' steps.\n",
    "- Always use provided selenium code that was already executed to find valid selectors for the final pytestfile. \n",
    "- You answer in python code only and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = \"\"\"\n",
    "import pytest\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pytest_bdd import scenarios, given, when, then\n",
    "# Constants\n",
    "BASE_URL = 'https://form.jotform.com/241472287797370'\n",
    "# Scenarios\n",
    "scenarios('test_form_submission.feature')\n",
    "# Fixtures\n",
    "@pytest.fixture\n",
    "def browser():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.get(BASE_URL)\n",
    "    yield driver\n",
    "    driver.quit()\n",
    "# Steps\n",
    "@given('I am on the job application page')\n",
    "def i_am_on_the_job_application_page(browser):\n",
    "    pass\n",
    "@when('I enter \"John\" in the \"First Name\" field')\n",
    "def i_enter_first_name(browser):\n",
    "    first_name_field = browser.find_element(By.XPATH, \"/html/body/form/div[1]/ul/li[2]/div/div/span[1]/input\")\n",
    "    first_name_field.send_keys(\"John\")\n",
    "@when('I enter \"Doe\" in the \"Last Name\" field')\n",
    "def i_enter_last_name(browser):\n",
    "    last_name_field = browser.find_element(By.XPATH, \"/html/body/form/div[1]/ul/li[2]/div/div/span[2]/input\")\n",
    "    last_name_field.send_keys(\"Doe\")\n",
    "@when('I enter \"john.doe@example.com\" in the \"Email Address\" field')\n",
    "def i_enter_email_address(browser):\n",
    "    email_field = browser.find_element(By.XPATH, \"/html/body/form/div[1]/ul/li[3]/div/span/input\")\n",
    "    email_field.send_keys(\"john.doe@example.com\")\n",
    "@when('I enter \"(123) 456-7890\" in the \"Phone Number\" field')\n",
    "def i_enter_phone_number(browser):\n",
    "    phone_number_field = browser.find_element(By.XPATH, \"/html/body/form/div[1]/ul/li[4]/div/span/input\")\n",
    "    phone_number_field.send_keys(\"(123) 456-7890\")\n",
    "@when('I leave the \"Cover Letter\" field empty')\n",
    "def i_leave_cover_letter_empty():\n",
    "    # No action needed as the field should remain empty\n",
    "    pass\n",
    "@when('I click the \"Apply\" button')\n",
    "def i_click_apply_button(browser):\n",
    "    apply_button = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"/html/body/form/div[1]/ul/li[6]/div/div/button\"))\n",
    "    )\n",
    "    browser.execute_script(\"arguments[0].scrollIntoView(true);\", apply_button)\n",
    "    apply_button.click()\n",
    "@then('I should see an error message for the \"Cover Letter\" field')\n",
    "def i_should_see_error_message(browser):\n",
    "    try:\n",
    "        error_message = browser.find_element(By.XPATH, \"/html/body/form/div[1]/ul/li[5]/div/div/span\")\n",
    "        assert error_message.is_displayed()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Error message not displayed: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "PROMPT = f\"\"\"{SYSTEM_PROMPT}\n",
    "\n",
    "Generate a valid pytest-bdd file with the following inputs and examples to guide you:\n",
    "Feature file name: {FEATURE_FILE_NAME}\n",
    "Gherkin feature: {generated_gherkin}\\n\n",
    "Already executed code:\\n{actions}\\n\n",
    "selected html of the last page: {nodes}\\n\n",
    "Examples:\\n\\n{EXAMPLES}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the automated test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_pytest = gpt4o.complete(PROMPT, image_documents=last_page_screenshot).text\n",
    "generated_pytest = clean_output(generated_pytest)\n",
    "print(generated_pytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup and write test to file\n",
    "\n",
    "We'll cleanup and write two generated files to disk: \n",
    "- `.feature` contains the test scenarios written in the Gherkin syntax\n",
    "- `.py` the actual automated test that we'll run with pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(FEATURE_FILE_NAME, \"w\") as file:\n",
    "        file.write(generated_gherkin)\n",
    "        \n",
    "with open(PYTEST_FILE_NAME, \"w\") as file:\n",
    "        print(\"WRITING FILE\")\n",
    "        file.write(generated_pytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -v {PYTEST_FILE_NAME}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavague-Oj4z07SL-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
